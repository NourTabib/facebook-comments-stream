{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy tqdm scikit-learn tensorflow tokenizers transformers"
      ],
      "metadata": {
        "id": "KKduSDooUyvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HE8iYfFc0ECn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "Ky0WeU4MxR68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg6iphRytUiP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "# from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import  matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from tokenizers import BertWordPieceTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk0_NQGRtUiR"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('tun.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msNe2J16tUiR"
      },
      "outputs": [],
      "source": [
        "data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcPbNnMWtUiS"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 100\n",
        "MAX_LEN = 192\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQID3Z5JtUiS"
      },
      "outputs": [],
      "source": [
        "def encode_text(texts, tokenizer, chunk_size=256, maxlen=512):\n",
        "\n",
        "    tokenizer.enable_truncation(max_length=maxlen)\n",
        "    tokenizer.enable_padding(length=maxlen)\n",
        "    all_ids = []\n",
        "    \n",
        "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
        "        text_chunk = texts[i:i+chunk_size].tolist()\n",
        "        encs = tokenizer.encode_batch(text_chunk)\n",
        "        all_ids.extend([enc.ids for enc in encs])\n",
        "    \n",
        "    return np.array(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjy7Z390tUiS"
      },
      "outputs": [],
      "source": [
        "fast_tokenizer = BertWordPieceTokenizer('/content/vocab.txt', lowercase=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABeeZ73LtUiS"
      },
      "outputs": [],
      "source": [
        "# from keras.optimizers_v1 import Adam\n",
        "# from tensorflow.compat.v1.keras import Adam\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "\n",
        "def build_model(transformer, max_len=512):\n",
        "\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "    \n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(tf.keras.optimizers.legacy.Adam(lr=1e-6), loss='binary_crossentropy', metrics=['accuracy','AUC'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ueb9mjH2tUiT"
      },
      "outputs": [],
      "source": [
        "texts = encode_text(data.text.values.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "ys = data.intent.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndGjkBeZtUiT"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def create_train(x_train,y_train) :\n",
        "    train_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices((x_train, y_train))\n",
        "        .repeat()\n",
        "        .shuffle(2048)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "    return  train_dataset\n",
        "\n",
        "def create_valid(x_valid,y_valid) :\n",
        "    valid_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices((x_valid, y_valid))\n",
        "        .batch(BATCH_SIZE)\n",
        "        .cache()\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "    \n",
        "    return valid_dataset\n",
        "\n",
        "def create_test(x_test) :\n",
        "    test_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices(x_test)\n",
        "        .batch(BATCH_SIZE)\n",
        "    )\n",
        "    return test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iexOJidntUiU"
      },
      "outputs": [],
      "source": [
        "FOLDS = 5\n",
        "SEED  = 42\n",
        "transformer_layer = (transformers.TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased'))\n",
        "model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "skf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\n",
        "\n",
        "for fold,(train_indices,valid_indices) in enumerate(skf.split(texts,ys)) :\n",
        "    print('Fold' , fold+1)\n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        'fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True,\n",
        "        save_weights_only=True, mode='min', save_freq='epoch')\n",
        "    \n",
        "    n_steps = train_indices.shape[0]\n",
        "    history = model.fit(\n",
        "    create_train(texts[train_indices],ys[train_indices]),\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=create_valid(texts[valid_indices],ys[valid_indices]),\n",
        "    epochs=EPOCHS,\n",
        "    callbacks =  [sv]    \n",
        "    )\n",
        "    \n",
        "    \n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.plot(np.arange(EPOCHS),history.history['auc'],'-o',label='Train AUC',color='#ff7f0e')\n",
        "    plt.plot(np.arange(EPOCHS),history.history['val_auc'],'-o',label='Val AUC',color='#1f77b4')\n",
        "    x = np.argmax( history.history['val_auc'] ); y = np.max( history.history['val_auc'] )\n",
        "    xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n",
        "    plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=14)\n",
        "    plt.legend(loc=2)\n",
        "    plt2 = plt.gca().twinx()\n",
        "    plt2.plot(np.arange(EPOCHS),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n",
        "    plt2.plot(np.arange(EPOCHS),history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n",
        "    x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n",
        "    ydist = plt.ylim()[1] - plt.ylim()[0]\n",
        "    plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss',size=14)\n",
        "    plt.ylabel('Loss',size=14)\n",
        "    plt.title('FOLD %i Distilbert-base-multilingual-cased'%\n",
        "                (fold+1),size=18)\n",
        "    plt.legend(loc=3)\n",
        "    plt.show()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPx5qSlwtUiU"
      },
      "source": [
        "### **Tests:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBm9-eGEtUiV"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('../input/vneuron/extra_test_data.csv')\n",
        "\n",
        "test_texts = encode_text(test.text.values.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
        "\n",
        "test_ys = test.intent.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLcI2PyvtUiV"
      },
      "outputs": [],
      "source": [
        "results = model.predict(create_test(test_texts))\n",
        "for i,result in enumerate(results) :\n",
        "    if result > 0.5 :\n",
        "        results[i] = 1\n",
        "    else :\n",
        "        results[i] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgVyyihbtUiW"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(test_ys, results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14C4tX1GtUiW"
      },
      "outputs": [],
      "source": [
        "print(classification_report(test_ys, results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp-f2f28tUiW"
      },
      "outputs": [],
      "source": [
        "all_ids = []\n",
        "encs = fast_tokenizer.encode_batch(['Yaatikom el saha','Wena mechi fel tari9','service khayeb'])\n",
        "all_ids.extend([enc.ids for enc in encs])\n",
        "test_data = create_test(np.array(all_ids))\n",
        "predictions = model.predict(test_data)\n",
        "for prediction in predictions :\n",
        "    print(prediction)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}